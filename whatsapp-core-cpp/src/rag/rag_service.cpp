#include "rag/rag_service.h"
#include "llm/ollama_client.h"
#include "rag/vector_store.h"
#include "persistence/repository.h"
#include <spdlog/spdlog.h>
#include <sstream>

void RagService::LoadHistoryFromDB() {
    spdlog::info("â³ Iniciando carga de historial en RAG (Esto puede tardar)...");
    
    // 1. Pedimos los Ãºltimos 50 mensajes (puedes cambiar el nÃºmero)
    // Si pones muchos, Ollama tardarÃ¡ bastante en generar los embeddings.
    auto history = m_db->GetAllMessages(1000); 

    int count = 0;
    int total = history.size();

    for (const auto& msg : history) {
        count++;
        // Usamos spdlog para mostrar progreso en la misma lÃ­nea si es posible, o log normal
        if (count % 5 == 0) spdlog::info("PROGRESO: Indexando {}/{}", count, total);

        // Llamamos a nuestra propia funciÃ³n de ingesta.
        // Como IngestMessage tiene mutex, es seguro.
        IngestMessage(msg.id, msg.content, msg.sender);
    }

    spdlog::info("âœ… Carga de historial completada. {} mensajes listos en memoria.", total);
}
// Constructor
RagService::RagService(std::shared_ptr<OllamaClient> llm, 
                       std::shared_ptr<VectorStore> v_store,
                       std::shared_ptr<Repository> db)
    : m_llm(llm), m_vec_store(v_store), m_db(db) {}

void RagService::IngestMessage(const std::string& msg_id, const std::string& content, const std::string& sender) {
    
    // =================================================================================
    // ðŸ”’ CANDADO DE SEGURIDAD (MUTEX)
    // =================================================================================
    // Esto obliga a que los mensajes se procesen UNO A UNO, aunque lleguen 100 a la vez.
    // Evita que FAISS explote por concurrencia y evita timeouts de Ollama.
    std::lock_guard<std::mutex> lock(m_ingest_mutex);
    // =================================================================================

    // 1. Validar limpieza (ignorar mensajes muy cortos)
    if (content.length() < 2) return;

    // 2. Crear texto enriquecido
    std::string text_to_embed = sender + ": " + content;

    // 3. Obtener vector de Ollama (Esto puede tardar 0.2s - 1s)
    auto embedding = m_llm->GetEmbedding(text_to_embed);

    // 4. Guardar en FAISS
    if (!embedding.empty()) {
        // NOTA: El orden suele ser (Vector, ID). Si tu VectorStore estÃ¡ al revÃ©s, cÃ¡mbialo aquÃ­.
        m_vec_store->AddIndex(msg_id, embedding); 
        spdlog::info("ðŸ§  Mensaje indexado en RAG: {}", msg_id);
    } else {
        spdlog::warn("âš ï¸ Fallo al generar embedding para mensaje: {}", msg_id);
    }
}

std::string RagService::Ask(const std::string& question) {
    spdlog::info("ðŸ¤– Usuario pregunta: {}", question);

    // 1. Vectorizar la pregunta (Usando Nomic idealmente)
    auto query_vec = m_llm->GetEmbedding(question);
    if (query_vec.empty()) return "Tuve un problema procesando tu pregunta.";

    // 2. Buscar contexto (Buscamos 8 para tener mÃ¡s margen de historia)
    auto relevant_ids = m_vec_store->Search(query_vec, 8);
    
    std::stringstream context_ss;
    bool found_data = false;

    // --- CONSTRUCCIÃ“N DEL CONTEXTO ---
    for (const auto& id : relevant_ids) {
        auto msg_text = m_db->GetMessageContentById(id); 
        if (!msg_text.empty()) {
            // Formateamos como lista para que el modelo lo lea fÃ¡cil
            context_ss << "- " << msg_text << "\n";
            found_data = true;
            
            // Log para debug (Vital para ver quÃ© encuentra)
            spdlog::info("ðŸ“„ RAG Contexto: {}", msg_text); 
        }
    }

    if (!found_data) return "No encontrÃ© informaciÃ³n relacionada en tus chats.";

    // =========================================================================
    // ðŸ§  SYSTEM PROMPT: EL CEREBRO DEL ASISTENTE
    // =========================================================================
    std::string system_prompt = 
        "Eres un Asistente de Memoria Personal inteligente y Ãºtil. "
        "Tu trabajo es responder a mis preguntas basÃ¡ndote en el historial de mis chats recuperados.\n"
        "\nINSTRUCCIONES CLAVE:"
        "\n1. INTERPRETACIÃ“N DE USUARIOS: El nombre 'Yo (Sistema)' o 'Yo' se refiere a MÃ (el usuario actual). Cuando hables de lo que dije, usa 'TÃº dijiste...'. Los otros nombres son mis contactos."
        "\n2. FUENTE DE VERDAD: Usa ÃšNICAMENTE el bloque de 'CONTEXTO'. Si la respuesta no estÃ¡ ahÃ­, di 'No recuerdo haber hablado de eso'."
        "\n3. CONTRADICCIONES: Si encuentras informaciÃ³n contradictoria (ej. dos colores favoritos o dos claves distintas), menciona AMBAS opciones indicando que aparecen en momentos diferentes."
        "\n4. PRIVACIDAD: Estos son mis datos personales. Si pregunto por un dato exacto (como una clave, direcciÃ³n o nÃºmero) que aparece en el contexto, tienes permiso para mostrÃ¡rmelo.";

    // =========================================================================
    // ðŸ—£ï¸ USER PROMPT: LA PETICIÃ“N
    // =========================================================================
    std::stringstream user_prompt_ss;
    user_prompt_ss 
        << "Consulta los siguientes fragmentos de mi historial de chat:\n\n"
        << "### CONTEXTO ###\n"
        << context_ss.str()
        << "### FIN CONTEXTO ###\n\n"
        << "Basado en lo anterior, responde: " << question;

    // 3. Enviar a Llama
    // Sugerencia: Usa temperatura baja (0.1 o 0.2) en OllamaClient para reducir alucinaciones
    auto answer = m_llm->Chat(system_prompt, user_prompt_ss.str());
    
    return answer.value_or("El modelo no pudo generar una respuesta.");
}